import asyncio
import logging
import re
import csv
import os
import requests
from datetime import datetime
from datetime import datetime as dt_datetime
from dotenv import load_dotenv
from weather_price_monitor import WeatherPriceMonitor
from engine.config import QuantConfig
from engine.data_feed import WeatherState
from engine.strategy import StrategyKernel
from engine.forecast_guard import ForecastGuardManager
from executor.poly_trader import PolyExecutor
from src.monitor.position_manager import PositionManager
from decimal import Decimal, ROUND_HALF_UP

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

import time

# å…¨å±€å¯åŠ¨æ—¶é—´ï¼Œç”¨äºé™é»˜æœŸåˆ¤æ–­
_STARTUP_TIME = time.time()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("WeatherBot")

_MONTH_NAME_TO_NUM = {
    "january": 1,
    "february": 2,
    "march": 3,
    "april": 4,
    "may": 5,
    "june": 6,
    "july": 7,
    "august": 8,
    "september": 9,
    "october": 10,
    "november": 11,
    "december": 12,
}

# é€šçŸ¥å†·å´ç¼“å­˜ (market, reason) -> last_send_time
_NOTIFICATION_COOLDOWN = {}

def send_dingtalk_notification(market, contract, price, shares, reason):
    """å‘é€é’‰é’‰äº¤æ˜“æœºä¼šé€šçŸ¥ (å¢åŠ å¯åŠ¨é™é»˜æœŸä¸æ¶ˆæ¯å»é‡)"""
    now = time.time()
    if now - _STARTUP_TIME < 60:
        logger.info(f"[é’‰é’‰] å¯åŠ¨é™é»˜æœŸï¼Œå¿½ç•¥é€šçŸ¥: {market} {reason}")
        return

    # [NEW] æ¶ˆæ¯å»é‡ logic: 6å°æ—¶å†…ç›¸åŒçš„å¸‚åœº+ç†ç”±åªå‘ä¸€æ¬¡ (é™¤éæ˜¯å®é™…æˆäº¤)
    # æˆäº¤é€šçŸ¥ shares > 0 åº”å½“æ€»æ˜¯å…è®¸å‘é€
    is_trade = shares > 0
    cache_key = (market, reason)
    if not is_trade:
        last_time = _NOTIFICATION_COOLDOWN.get(cache_key, 0)
        if now - last_time < 21600: # 6 hours
            logger.info(f"[é’‰é’‰] æ¶ˆæ¯å¤„äºå†·å´æœŸï¼Œè·³è¿‡é‡å¤é€šçŸ¥: {market} {reason}")
            return
    
    webhook = os.getenv("DINGTALK_WEBHOOK")
    if not webhook:
        logger.warning("é’‰é’‰ Webhook æœªé…ç½®ï¼Œè·³è¿‡é€šçŸ¥")
        return
    
    total_cost = price * shares
    
    # æ¶ˆæ¯éœ€è¦åŒ…å«å…³é”®è¯ "beijixing" æˆ–å…¶ä»–å·²è®¾å®šçš„å…³é”®è¯
    message = f"""[Beijixing-WeatherBot] ğŸš¨ Polymarket äº¤æ˜“è§¦å‘æé†’

ğŸ“ å¸‚åœº: {market}
ğŸ¯ ç›®æ ‡åˆçº¦: {contract}
ğŸ’° ä¹°å…¥å•ä»·: {price:.3f} USDC
æŒæœ‰ä»½é¢: {shares:.1f}
æ€»è®¡æˆæœ¬: {total_cost:.2f} USDC
ğŸ“ è§¦å‘ç†ç”±: {reason}
â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è¯·åŠæ—¶å…³æ³¨å®ç›˜åŠ¨æ€ï¼
-- [Robot: Weather Bot]"""
    
    payload = {
        "msgtype": "text",
        "text": {"content": message}
    }
    
    import json
    logger.info(f"[é’‰é’‰æ¨é€] Payload: {json.dumps(payload, ensure_ascii=False)}")
    
    try:
        resp = requests.post(webhook, json=payload, timeout=5)
        if resp.status_code == 200:
            logger.info(f"[é’‰é’‰] é€šçŸ¥å‘é€æˆåŠŸ")
            if not is_trade:
                _NOTIFICATION_COOLDOWN[cache_key] = now
        else:
            logger.warning(f"[é’‰é’‰] é€šçŸ¥å‘é€å¤±è´¥: {resp.text}")
    except Exception as e:
        logger.error(f"[é’‰é’‰] é€šçŸ¥å‘é€å¼‚å¸¸: {e}")


def send_fg_lock_dingtalk_notification(market, fg_reason, risk_count, available_sources, risky_sources):
    """å‘é€ ForecastGuard é”ä»“é€šçŸ¥ï¼ˆä»…åœ¨é”ä»“äº‹ä»¶è§¦å‘æ—¶è°ƒç”¨ï¼‰"""
    now = time.time()
    if now - _STARTUP_TIME < 60:
        logger.info(f"[é’‰é’‰] å¯åŠ¨é™é»˜æœŸï¼Œå¿½ç•¥ FG é”ä»“é€šçŸ¥: {market}")
        return

    reason_text = fg_reason or "ForecastGuard locked"
    cache_key = (market, "FG_LOCK", reason_text)
    last_time = _NOTIFICATION_COOLDOWN.get(cache_key, 0)
    if now - last_time < 21600:  # 6 hours
        logger.info(f"[é’‰é’‰] FG é”ä»“é€šçŸ¥å¤„äºå†·å´æœŸï¼Œè·³è¿‡: {market} {reason_text}")
        return

    webhook = os.getenv("DINGTALK_WEBHOOK")
    if not webhook:
        logger.warning("é’‰é’‰ Webhook æœªé…ç½®ï¼Œè·³è¿‡ FG é”ä»“é€šçŸ¥")
        return

    risky_text = ", ".join(risky_sources) if risky_sources else "N/A"
    message = f"""[Beijixing-WeatherBot] âš ï¸ ForecastGuard é”ä»“é€šçŸ¥

ğŸ“ å¸‚åœº: {market}
ğŸ”’ çŠ¶æ€: FG_LOCKED
ğŸ“Š é£é™©æº: {risk_count}/{available_sources}
ğŸ§© é£é™©æ¥æº: {risky_text}
ğŸ“ åŸå› : {reason_text}
â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

-- [Robot: Weather Bot]"""

    payload = {
        "msgtype": "text",
        "text": {"content": message}
    }

    import json
    logger.info(f"[FGé’‰é’‰æ¨é€] Payload: {json.dumps(payload, ensure_ascii=False)}")

    try:
        resp = requests.post(webhook, json=payload, timeout=5)
        if resp.status_code == 200:
            _NOTIFICATION_COOLDOWN[cache_key] = now
            logger.info(f"[é’‰é’‰] FG é”ä»“é€šçŸ¥å‘é€æˆåŠŸ")
        else:
            logger.warning(f"[é’‰é’‰] FG é”ä»“é€šçŸ¥å‘é€å¤±è´¥: {resp.text}")
    except Exception as e:
        logger.error(f"[é’‰é’‰] FG é”ä»“é€šçŸ¥å‘é€å¼‚å¸¸: {e}")


class WeatherBot:
    """å¹¶è¡Œå¤šåŒºåŸŸäº¤æ˜“æœºå™¨äºº"""
    
    def __init__(self):
        self.config = QuantConfig
        self.executor = PolyExecutor(self.config)
        self.pos_manager = PositionManager()
        self.forecast_guard = ForecastGuardManager(self.config)
        
    def _get_local_time_info(self, offset):
        """è·å–ç«™ç‚¹æœ¬åœ°æ—¶é—´ä¿¡æ¯: (å°æ—¶æµ®ç‚¹æ•°, HH:MM å­—ç¬¦ä¸²)"""
        import datetime as dt
        from datetime import timezone, timedelta
        utc_now = dt.datetime.now(timezone.utc)
        local_time = utc_now + timedelta(hours=offset)
        hour_float = local_time.hour + local_time.minute / 60.0
        time_str = local_time.strftime("%H:%M")
        return hour_float, time_str

    async def run_location_loop(self, preset_name, interval=60):
        """å•ä¸ªåœ°ç‚¹çš„ç›‘å¬ä¸å†³ç­–é—­ç¯ (æ”¯æŒè·¨å¤©è‡ªåŠ¨åˆ‡æ¢)"""
        from weather_price_monitor import PRESETS
        
        if preset_name not in PRESETS:
            logger.error(f"Preset '{preset_name}' not found!")
            return
            
        conf = PRESETS[preset_name]
        tz_offset = conf.get("tz_offset", 0)
        
        # åˆå§‹æ—¥æœŸä¸å½•åˆ¶æ–‡ä»¶å
        current_date_str = self._get_local_date(tz_offset)
        slug = self._get_dynamic_slug(conf['slug_template'], tz_offset)
        
        # å½•åˆ¶æ–‡ä»¶åæ ¼å¼: weather_recording_{city}_{YYYYMMDD}_{HHMM}.csv
        session_start = datetime.now().strftime('%Y%m%d_%H%M')
        current_recording_file = f"data/recordings/weather_recording_{preset_name}_{session_start}.csv"
        
        monitor = WeatherPriceMonitor(
            conf["icao"], slug, conf["lat"], conf["lon"]
        )
        # ä¸º monitor æŒ‡å®š CSV æ–‡ä»¶ï¼ˆè™½ç„¶ hub ä¹Ÿä¼šè®°å½•ï¼Œä½†ä¿æŒä¸€è‡´æ€§ï¼‰
        monitor.csv_file = current_recording_file
        
        # æ¯ä¸ªåœ°ç‚¹ç»´æŠ¤ç‹¬ç«‹çš„ç‰©ç†çŠ¶æ€
        local_hour, local_time_str = self._get_local_time_info(tz_offset)
        state = WeatherState(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            local_time=local_time_str,
            local_hour=local_hour
        )
        
        logger.info(f"[+] Loop Started: {preset_name} | Slug: {slug} | TZ: {tz_offset}")

        # [NEW] æ—¥å¿—èŠ‚æµæ§åˆ¶
        last_status_log_time = 0
        last_status_summary = ""
        
        unit = conf.get("unit", "C")
        
        # [NEW] å¯åŠ¨æ—¶å°è¯•æ¢å¤å½“æ—¥å†å²æœ€é«˜æ¸©
        recovered_max = self._recover_today_max_temp(preset_name, current_date_str)
        if recovered_max is not None:
            state.max_temp_overall = recovered_max
            logger.info(f"[{preset_name:8}] ğŸ’¾ æˆåŠŸä»å†å²è®°å½•æ¢å¤å½“æ—¥æœ€é«˜æ¸©: {recovered_max:.2f}")

        # [NEW] å¯åŠ¨æ—¶å°è¯•æ¢å¤å½“æ—¥äº¤æ˜“çŠ¶æ€ (é˜²æ­¢é‡å¯åé‡å¤ä¸‹å•)
        if self._recover_today_trade_status(preset_name, current_date_str, tz_offset):
            state.has_traded_today = True
            logger.info(f"[{preset_name:8}] ğŸ”’ æˆåŠŸä»å†å²è®°å½•æ¢å¤å·²äº¤æ˜“çŠ¶æ€ (Has Traded Today)")

        # FG é”ä»“çŠ¶æ€æœºï¼šä»…åœ¨é”ä»“çŠ¶æ€åˆ‡æ¢æ—¶å‘é€šçŸ¥ï¼Œé¿å…å¾ªç¯å†…é‡å¤æ¨é€
        prev_fg_locked = False
        prev_fg_reason = ""
        
        while True:
            try:
                # æ£€æŸ¥æ—¥æœŸï¼Œå¦‚æœè·¨å¤©åˆ™åˆ·æ–° slug
                now_date_str = self._get_local_date(tz_offset)
                if now_date_str != current_date_str:
                    # è·¨å¤©å‰å°†ä¸Šä¸€äº¤æ˜“æ—¥ outcome æ ‡è®°ä¸ºæœ€ç»ˆç»“ç®—
                    if state.max_temp_overall > -900:
                        self._upsert_outcome_row(
                            preset_name=preset_name,
                            date_str=current_date_str,
                            slug_id=slug,
                            noaa_max=state.max_temp_overall,
                            is_final=True,
                        )
                        logger.info(
                            f"[{preset_name:8}] ğŸ§¾ Outcome finalized for {current_date_str} | Max(NOAA): {state.max_temp_overall:.2f}"
                        )
                    logger.info(f"[{preset_name:8}] Date changed ({current_date_str} -> {now_date_str}), refreshing slug...")
                    current_date_str = now_date_str
                    slug = self._get_dynamic_slug(conf['slug_template'], tz_offset)
                    monitor.poly_url = f"https://gamma-api.polymarket.com/events?slug={slug}"
                    monitor.event_slug = slug
                    # [å…³é”®] å¼ºåˆ¶åˆ·æ–° CSV æ–‡ä»¶ä»¥å¯¹åº”æ–°äº¤æ˜“æ—¥
                    session_start = datetime.now().strftime('%Y%m%d_%H%M')
                    current_recording_file = f"data/recordings/weather_recording_{preset_name}_{session_start}.csv"
                    monitor.csv_file = current_recording_file
                    # é‡ç½®çŠ¶æ€
                    state.has_traded_today = False
                    state.max_temp_overall = -999.0
                    state.v_fit_history = []
                    logger.info(f"[{preset_name:8}] New Session: {slug} | File: {current_recording_file}")

                    # [NEW] è·¨å¤©åå†æ¬¡æ£€æŸ¥å½“æ—¥äº¤æ˜“çŠ¶æ€ (é˜²æ­¢è·¨å¤©é‡å¯è¾¹ç¼˜case)
                    if self._recover_today_trade_status(preset_name, current_date_str, tz_offset):
                        state.has_traded_today = True
                        logger.info(f"[{preset_name:8}] ğŸ”’ è·¨å¤©æ£€æµ‹åˆ°ä»Šæ—¥å·²æœ‰äº¤æ˜“è®°å½• (Has Traded Today)")

                # 1. è·å–æœ¬åœ°æ—¶é—´ä¸æ•°æ® (æ³¨å…¥å·®å¼‚åŒ–é‡‡æ ·é—´éš”)
                state.local_hour, state.local_time = self._get_local_time_info(tz_offset)
                loop = asyncio.get_event_loop()
                wd = await loop.run_in_executor(
                    None, 
                    lambda: monitor.fetch_all_sources(
                        om_interval=self.config.INTERVAL_OM, 
                        mn_interval=self.config.INTERVAL_MN
                    )
                )
                prices = await loop.run_in_executor(None, monitor.fetch_polymarket_asks)
                
                # 2. çŠ¶æ€å½•åƒ
                state.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                state.noaa_curr = wd['sources']['NOAA (METAR)']['curr']
                state.om_curr = wd['sources']['Open-Meteo']['curr']
                state.om_fore = wd['sources']['Open-Meteo']['fore']
                state.mn_curr = wd['sources']['Met.no']['curr']
                state.mn_fore = wd['sources']['Met.no']['fore']
                state.consensus_curr = wd['avg_curr']
                state.consensus_fore = wd['avg_fore']
                
                if state.noaa_curr is not None:
                    prev_noaa_max = state.max_temp_overall
                    state.max_temp_overall = max(state.max_temp_overall, state.noaa_curr)
                    if state.max_temp_overall > (prev_noaa_max + 1e-9):
                        # ç›˜ä¸­å®æ—¶è´¦æœ¬ï¼šå½“å¤©å‡ºç°æ–°é«˜æ—¶ upsert åŒä¸€å¤©è®°å½•
                        self._upsert_outcome_row(
                            preset_name=preset_name,
                            date_str=current_date_str,
                            slug_id=slug,
                            noaa_max=state.max_temp_overall,
                            is_final=False,
                        )
                if state.om_curr is not None:
                    state.max_temp_om = max(state.max_temp_om, state.om_curr)
                if state.mn_curr is not None:
                    state.max_temp_mn = max(state.max_temp_mn, state.mn_curr)
                
                # ç»´æŠ¤è¿ç»­ä¸‹è·Œè®¡æ•° (NOAA æ ¸å¿ƒåŸºå‡†)
                if state.noaa_curr is not None and state.max_temp_overall > -900:
                    if state.noaa_curr < state.max_temp_overall:
                        state.drop_count += 1
                    else:
                        state.drop_count = 0
                
                if state.om_curr is not None: state.om_history.append(state.om_curr)
                if state.mn_curr is not None: state.mn_history.append(state.mn_curr)
                if state.noaa_curr is not None: state.noaa_history.append(state.noaa_curr)
                
                v_fit = (state.om_curr * self.config.W1_OM + state.mn_curr * self.config.W2_MN) if (state.om_curr and state.mn_curr) else None
                if v_fit:
                    state.update_v_fit(v_fit)
                
                for hist in [state.noaa_history, state.om_history, state.mn_history, state.v_fit_history]:
                    if len(hist) > 10: hist.pop(0)

                # 3. ç­–ç•¥å†³ç­–
                state.market_prices = prices # å­˜å…¥å…¨é‡æŠ¥ä»·

                guard_state = self.forecast_guard.assess(preset_name, state, conf)

                fg_locked_now = bool(guard_state.get("locked"))
                fg_reason_now = guard_state.get("reason", "")
                if fg_locked_now and (not prev_fg_locked or fg_reason_now != prev_fg_reason):
                    src_reports = guard_state.get("sources", {}) if isinstance(guard_state.get("sources"), dict) else {}
                    risky_sources = sorted(
                        [
                            name for name, rep in src_reports.items()
                            if isinstance(rep, dict) and rep.get("risky")
                        ]
                    )
                    send_fg_lock_dingtalk_notification(
                        market=preset_name.upper(),
                        fg_reason=fg_reason_now,
                        risk_count=int(guard_state.get("risk_count", 0)),
                        available_sources=int(guard_state.get("available_sources", 0)),
                        risky_sources=risky_sources
                    )
                prev_fg_locked = fg_locked_now
                prev_fg_reason = fg_reason_now
                
                # --- æ–°é€»è¾‘ï¼šNOAA ä¸‹è·Œè§¦å‘ / 17ç‚¹å¼ºä¹° ---
                signal, reason, target_temp = StrategyKernel.calculate_noaa_drop_signal(
                    state,
                    self.config,
                    state.max_temp_overall if state.max_temp_overall > -900 else None,
                    state.has_traded_today,
                    forecast_guard=guard_state,
                )
                
                # å…¼å®¹åŸæœ‰ v_fit æ˜¾ç¤º
                # 3.5 æ—¥å¿—èŠ‚æµé€»è¾‘ (åŒä¸€çŠ¶æ€ 60s æ‰“å°ä¸€æ¬¡)
                guard_tag = "LOCKED" if guard_state.get("locked") else "PASS"
                status_summary = f"{guard_tag}({guard_state.get('risk_count', 0)}/{guard_state.get('available_sources', 0)}) | {signal:5}"
                now_ts = time.time()
                if status_summary != last_status_summary or (now_ts - last_status_log_time) > 60:
                    logger.info(
                        f"[{preset_name:8}] NOAA: {state.noaa_curr if state.noaa_curr else 0.0:.1f} | "
                        f"Max: {state.max_temp_overall if state.max_temp_overall > -900 else 0.0:.1f} | "
                        f"FG: {status_summary} | Reason: {reason}"
                    )
                    last_status_log_time = now_ts
                    last_status_summary = status_summary

                # 4. æ‰§è¡Œå†³ç­– (Dry Run æˆ– Real)
                if signal in ['BUY_DROP', 'BUY_FORCE']:
                    # ç¡®å®šåˆçº¦ï¼šä½¿ç”¨è§¦å‘æ—¶çš„æ¸©åº¦ (target_temp)
                    # [Unit Conversion] å¦‚æœå•ä½æ˜¯åæ°åº¦ï¼Œè¿›è¡Œè½¬æ¢
                    display_temp = target_temp
                    symbol = "Â°C"
                    if unit == "F" and target_temp is not None:
                        # NWS standard: Round Half Up (Asymmetric)
                        f_temp = target_temp * 1.8 + 32
                        display_temp = int(Decimal(str(f_temp)).quantize(Decimal("1"), rounding=ROUND_HALF_UP))
                        symbol = "Â°F"
                    
                    target_contract_prefix = f"{int(display_temp)}{symbol}"
                    
                    # æ™ºèƒ½æœç´¢åˆçº¦ (å¤„ç† NYC çš„èŒƒå›´åˆçº¦ï¼Œå¦‚ "24-25Â°F")
                    target_contract = target_contract_prefix # é»˜è®¤
                    contract_price = None
                    
                    if prices:
                        found = False
                        for title in prices.keys():
                            # 1. å‰ç¼€/åŒ…å«åŒ¹é…
                            if target_contract_prefix in title:
                                target_contract = title
                                found = True
                                break
                            
                            # 2. åæ°åº¦èŒƒå›´åŒ¹é… (NYC ä¸“ç”¨)
                            if unit == 'F':
                                # åŒ¹é… "X-YÂ°F"
                                range_match = re.search(r'(\d+)-(\d+)Â°F', title)
                                if range_match:
                                    low, high = int(range_match.group(1)), int(range_match.group(2))
                                    if low <= display_temp <= high:
                                        target_contract = title
                                        found = True
                                        break
                                # åŒ¹é… "XÂ°F or below"
                                below_match = re.search(r'(\d+)Â°F or below', title)
                                if below_match and display_temp <= int(below_match.group(1)):
                                    target_contract = title
                                    found = True
                                    break
                                # åŒ¹é… "XÂ°F or higher"
                                higher_match = re.search(r'(\d+)Â°F or higher', title)
                                if higher_match and display_temp >= int(higher_match.group(1)):
                                    target_contract = title
                                    found = True
                                    break
                        
                        p_data = prices.get(target_contract)
                        if p_data:
                            contract_price = p_data.get('yes_ask') if isinstance(p_data, dict) else p_data
                    
                    # [Rule] ä»·æ ¼ä¿æŠ¤ï¼šdry run / real ç»Ÿä¸€è¦æ±‚ ask >= MIN_YES_ASK
                    should_execute = True
                    price_val = float(contract_price) if contract_price else 0.0
                    min_yes_ask = float(self.config.MIN_YES_ASK)
                    
                    if price_val + 1e-9 < min_yes_ask:
                        should_execute = False
                        reason_prefix = "Force buy" if signal == 'BUY_FORCE' else "Drop buy"
                        reason = f"{reason_prefix} skipped: Price {price_val:.3f} < MinAsk {min_yes_ask:.3f}"
                        logger.info(f"[{preset_name:8}] {reason} (Contract: {target_contract})")
                        
                        # å‘é€è·³è¿‡é€šçŸ¥
                        send_dingtalk_notification(
                            market=preset_name.upper(),
                            contract=target_contract,
                            price=price_val,
                            shares=0, # No shares bought
                            reason=reason
                        )
                        # æ ‡è®°ä¸ºå·²å®Œæˆï¼ˆé¿å…é‡å¤å°è¯•ï¼‰
                        state.has_traded_today = True

                        # 5. ç‹¬ç«‹äº¤æ˜“å­˜è¯ (New Feature)
                        self._record_trade_event(
                            preset_name=preset_name,
                            city_name=conf.get("city_name", preset_name),
                            local_time=state.local_time,
                            signal=f"SKIP_{signal.split('_')[1]}",
                            slug=slug,
                            contract=target_contract,
                            price=price_val,
                            shares=0,
                            reason=reason
                        )
                            
                    if should_execute:
                        # å‘é€äº¤æ˜“é€šçŸ¥
                        send_dingtalk_notification(
                            market=preset_name.upper(),
                            contract=target_contract,
                            price=price_val,
                            shares=self.config.TRADE_SHARES,
                            reason=reason
                        )
                        
                        # åªæœ‰åœ¨é Dry Run ä¸”æ‹¿åˆ°ä»·æ ¼æ—¶æ‰ä¸‹å•
                        if not self.config.DRY_RUN and contract_price:
                            # è½¬æ¢ä¿¡å·ä¸ºæ ‡å‡† BUY è¿›è¡Œæ‰§è¡Œ
                            await self.executor.execute_trade('BUY', monitor.event_slug, contract_price, self.config.TRADE_SHARES)
                        
                        # æ ‡è®°ä»Šæ—¥å·²äº¤æ˜“
                        state.has_traded_today = True
                        logger.info(f"[{preset_name:8}] âš¡ Trade Triggered ({signal}). Daily trade locked.")

                        # è®°å½•äº¤æ˜“äº‹ä»¶å¹¶å¼€å§‹è¿½è¸ªç”Ÿå‘½å‘¨æœŸ
                        order_id = f"dry_{int(datetime.now().timestamp())}"
                        if not self.config.DRY_RUN:
                            # å®é™…ä¸‹å•æ—¶åº”ä»æ‰§è¡Œå™¨è·å–çœŸå® OrderID
                            # è¿™é‡Œå‡è®¾æ‰§è¡Œå™¨è¿”å›ç»“æœä¸­åŒ…å« order_id
                            pass 

                        self.pos_manager.record_pending_order(
                            city_name=conf.get("city_name", preset_name),
                            local_time=state.local_time,
                            signal=signal,
                            slug=slug,
                            contract=target_contract,
                            price=price_val,
                            shares=self.config.TRADE_SHARES,
                            reason=reason,
                            order_id=order_id,
                            is_dry_run=self.config.DRY_RUN
                        )
                
                self._record_data(current_recording_file, state, prices, signal, reason, guard_state)

            except Exception as e:
                logger.error(f"[{preset_name}] Loop error: {e}")
                
            await asyncio.sleep(interval)

    def _get_local_date(self, offset):
        """è·å–ç«™ç‚¹æœ¬åœ°æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)"""
        import datetime as dt
        from datetime import timezone, timedelta
        utc_now = dt.datetime.now(timezone.utc)
        return (utc_now + timedelta(hours=offset)).strftime("%Y-%m-%d")

    def _recover_today_max_temp(self, preset_name, date_str):
        """æ¢å¤å½“æ—¥æœ€é«˜æ¸©ï¼šä¼˜å…ˆ outcome è´¦æœ¬ï¼Œå¤±è´¥åå›é€€æ‰«æ recording"""
        outcome_max = self._recover_today_max_from_outcome(preset_name, date_str)
        if outcome_max is not None:
            return outcome_max

        import glob
        # è½¬æ¢ 2026-02-11 ä¸º 20260211
        search_date = date_str.replace("-", "")
        pattern = f"data/recordings/weather_recording_{preset_name}_{search_date}_*.csv"
        files = sorted(glob.glob(pattern))
        
        if not files:
            return None
            
        max_val = -999.0
        found = False
        
        # éå†ä»Šæ—¥æ‰€æœ‰æ–‡ä»¶ï¼ˆé˜²æ­¢é‡å¯å¤šæ¬¡äº§ç”Ÿå¤šä¸ªæ–‡ä»¶ï¼‰
        for fpath in files:
            try:
                with open(fpath, mode='r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        raw_val = row.get('noaa_curr')
                        if not raw_val or raw_val == 'N/A':
                            # å…¼å®¹æ—§å­—æ®µå
                            raw_val = row.get('noaa_temp')
                        if raw_val and raw_val != 'N/A':
                            try:
                                val = float(raw_val)
                                if val > max_val:
                                    max_val = val
                                    found = True
                            except ValueError:
                                continue
            except Exception as e:
                logger.warning(f"[{preset_name:8}] æ¢å¤å†å²æ–‡ä»¶å¤±è´¥ {fpath}: {e}")

        return max_val if found else None

    def _recover_today_max_from_outcome(self, preset_name, date_str):
        """ä» outcome è´¦æœ¬æ¢å¤å½“æ—¥æœ€é«˜æ¸© (is_final TRUE/FALSE å‡å¯)"""
        filename = self._get_outcome_filename(preset_name)
        if not os.path.exists(filename):
            return None

        max_val = -999.0
        found = False
        try:
            with open(filename, mode='r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('date', '')).strip() != date_str:
                        continue
                    val = self._safe_float(row.get('noaa_max'))
                    if val is None:
                        continue
                    if val > max_val:
                        max_val = val
                        found = True
        except Exception as e:
            logger.warning(f"[{preset_name:8}] è¯»å– outcome æ¢å¤å¤±è´¥ {filename}: {e}")

        return max_val if found else None

    def _recover_today_trade_status(self, city_name, date_str, tz_offset):
        """æ£€æŸ¥ä»Šæ—¥äº¤æ˜“è®°å½•æ–‡ä»¶ï¼Œåˆ¤æ–­æ˜¯å¦å·²å®Œæˆäº¤æ˜“ (é˜²æ­¢é‡å¯åé‡å¤ä¸‹å•)"""
        # Dry run ä¸è¿›è¡ŒçœŸå®äº¤æ˜“çŠ¶æ€æ¢å¤ï¼Œé¿å…æµ‹è¯•æ•°æ®å½±å“é€»è¾‘ã€‚
        if self.config.DRY_RUN:
            return False

        # äº¤æ˜“è®°å½•æ–‡ä»¶: data/trades/trade_history_{city}.csv
        filename = f"data/trades/trade_history_{city_name}.csv"
        if not os.path.exists(filename):
            return False
            
        target_day = None
        try:
            target_day = dt_datetime.strptime(date_str, "%Y-%m-%d").date()
        except ValueError:
            return False

        try:
            with open(filename, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if str(row.get('is_dry_run', 'FALSE')).upper() == 'TRUE':
                        continue
                    try:
                        shares = float(row.get('shares', 0) or 0)
                    except ValueError:
                        shares = 0.0
                    if shares <= 0:
                        continue

                    status = str(row.get('status', '')).upper()
                    signal_type = str(row.get('signal_type', '')).upper()
                    is_trade_row = (status in {'PENDING', 'FILLED', 'WIN', 'LOSS', 'REDEEMED'}) or signal_type.startswith('BUY')
                    if not is_trade_row:
                        continue

                    slug = row.get('contract_slug', '')
                    if self._slug_matches_local_date(slug, target_day):
                        return True
        except Exception as e:
            logger.error(f"Error checking trade history for {city_name}: {e}")
            
        return False

    @staticmethod
    def _slug_matches_local_date(slug: str, target_day) -> bool:
        # slug å½¢å¦‚: highest-temperature-in-seoul-on-february-12-2026
        if not slug:
            return False
        m = re.search(r'on-([a-z]+)-(\d{1,2})-(\d{4})$', slug.lower())
        if not m:
            return False
        month_name, day_str, year_str = m.group(1), m.group(2), m.group(3)
        month = _MONTH_NAME_TO_NUM.get(month_name)
        if month is None:
            return False
        try:
            slug_day = dt_datetime(int(year_str), month, int(day_str)).date()
        except ValueError:
            return False
        return slug_day == target_day

    def _record_outcome(self, preset_name, date_str, slug_id, state):
        """ç»“ç®—å¹¶è®°å½•æ¯æ—¥æœ€ç»ˆç»“æœ (Outcome)"""
        if state.max_temp_overall <= -900:
            return
        self._upsert_outcome_row(
            preset_name=preset_name,
            date_str=date_str,
            slug_id=slug_id,
            noaa_max=state.max_temp_overall,
            is_final=True,
        )
        logger.info(f"[âœ“] Daily Summary Saved for {preset_name} | Max(NOAA): {state.max_temp_overall:.2f}")
        # é‡ç½®æœ€é«˜æ¸©ä»¥ä¾¿ç¬¬äºŒå¤©é‡æ–°å¼€å§‹
        state.max_temp_overall = -999.0

    @staticmethod
    def _safe_float(raw_val):
        if raw_val is None:
            return None
        txt = str(raw_val).strip()
        if not txt or txt.upper() == "N/A":
            return None
        try:
            return float(txt)
        except (TypeError, ValueError):
            return None

    @staticmethod
    def _parse_bool_str(raw_val) -> bool:
        if raw_val is None:
            return False
        return str(raw_val).strip().upper() in {"1", "TRUE", "YES", "Y"}

    @staticmethod
    def _format_noaa_max(noaa_max):
        if noaa_max is None:
            return ""
        return f"{float(noaa_max):.2f}"

    @staticmethod
    def _outcome_fieldnames():
        # ä¿æŒä¸å†å²æ–‡ä»¶å…¼å®¹ï¼šæ—§åˆ—ç»§ç»­ä¿ç•™ï¼Œæ–°å¢ is_final
        return ["date", "slug_id", "target_threshold", "noaa_max", "result", "is_final"]

    def _get_outcome_filename(self, preset_name):
        data_dir = "data/outcomes"
        os.makedirs(data_dir, exist_ok=True)
        return f"{data_dir}/outcome_{preset_name}.csv"

    def _atomic_write_csv(self, filename, fieldnames, rows):
        tmp_filename = f"{filename}.tmp.{os.getpid()}"
        try:
            with open(tmp_filename, "w", newline="", encoding="utf-8") as fw:
                writer = csv.DictWriter(fw, fieldnames=fieldnames)
                writer.writeheader()
                for row in rows:
                    writer.writerow({k: row.get(k, "") for k in fieldnames})
            os.replace(tmp_filename, filename)
        finally:
            if os.path.exists(tmp_filename):
                try:
                    os.remove(tmp_filename)
                except OSError:
                    pass

    def _upsert_outcome_row(self, preset_name, date_str, slug_id, noaa_max, is_final=False, target_threshold="", result=""):
        """æŒ‰ date å¯¹ outcome æ–‡ä»¶æ‰§è¡ŒåŸå­ upsertï¼Œé¿å…åŒä¸€å¤©é‡å¤è¿½åŠ å¤šè¡Œã€‚"""
        filename = self._get_outcome_filename(preset_name)
        fieldnames = self._outcome_fieldnames()

        def _normalize_row(row):
            normalized = {k: str(row.get(k, "")).strip() for k in fieldnames}
            if self._parse_bool_str(normalized.get("is_final")):
                normalized["is_final"] = "TRUE"
            else:
                normalized["is_final"] = "FALSE"
            return normalized

        rows_by_date = {}
        ordered_dates = []
        if os.path.exists(filename):
            try:
                with open(filename, mode="r", encoding="utf-8") as fr:
                    reader = csv.DictReader(fr)
                    for raw_row in reader:
                        date_key = str(raw_row.get("date", "")).strip()
                        if not date_key:
                            continue
                        row = _normalize_row(raw_row)
                        prev = rows_by_date.get(date_key)
                        if prev is None:
                            rows_by_date[date_key] = row
                            ordered_dates.append(date_key)
                        else:
                            prev_max = self._safe_float(prev.get("noaa_max"))
                            new_max = self._safe_float(row.get("noaa_max"))
                            if new_max is not None and (prev_max is None or new_max > prev_max):
                                prev["noaa_max"] = self._format_noaa_max(new_max)
                            if row.get("slug_id"):
                                prev["slug_id"] = row["slug_id"]
                            if row.get("target_threshold"):
                                prev["target_threshold"] = row["target_threshold"]
                            if row.get("result"):
                                prev["result"] = row["result"]
                            prev["is_final"] = "TRUE" if (
                                self._parse_bool_str(prev.get("is_final")) or self._parse_bool_str(row.get("is_final"))
                            ) else "FALSE"
            except Exception as e:
                logger.warning(f"[{preset_name:8}] è¯»å– outcome æ–‡ä»¶å¤±è´¥ï¼Œæ”¹ä¸ºé‡å»º {filename}: {e}")
                rows_by_date = {}
                ordered_dates = []

        incoming = {
            "date": date_str,
            "slug_id": slug_id or "",
            "target_threshold": target_threshold or "",
            "noaa_max": self._format_noaa_max(noaa_max if (noaa_max is not None and noaa_max > -900) else None),
            "result": result or "",
            "is_final": "TRUE" if is_final else "FALSE",
        }
        prev = rows_by_date.get(date_str)
        if prev is None:
            rows_by_date[date_str] = incoming
            ordered_dates.append(date_str)
        else:
            prev_max = self._safe_float(prev.get("noaa_max"))
            incoming_max = self._safe_float(incoming.get("noaa_max"))
            if incoming_max is not None and (prev_max is None or incoming_max > prev_max):
                prev["noaa_max"] = self._format_noaa_max(incoming_max)
            if incoming.get("slug_id"):
                prev["slug_id"] = incoming["slug_id"]
            if incoming.get("target_threshold"):
                prev["target_threshold"] = incoming["target_threshold"]
            if incoming.get("result"):
                prev["result"] = incoming["result"]
            prev["is_final"] = "TRUE" if (
                self._parse_bool_str(prev.get("is_final")) or self._parse_bool_str(incoming.get("is_final"))
            ) else "FALSE"

        ordered_rows = [rows_by_date[d] for d in ordered_dates if d in rows_by_date]
        self._atomic_write_csv(filename, fieldnames, ordered_rows)

    def _get_dynamic_slug(self, template, offset):
        """æ ¹æ®ç«™ç‚¹æœ¬åœ°æ—¶é—´åŠ¨æ€ç”Ÿæˆ Slug"""
        import datetime as dt
        from datetime import timezone, timedelta
        
        utc_now = dt.datetime.now(timezone.utc)
        local_time = utc_now + timedelta(hours=offset)
        
        # Polymarket æ ¼å¼: month-day-year (lowercase, e.g. february-5-2026)
        month_name = local_time.strftime("%B").lower()
        day = local_time.day
        year = local_time.year
        
        return template.format(month=month_name, day=day, year=year)

        return template.format(month=month_name, day=day, year=year)

    def _record_trade_event(self, preset_name, city_name, local_time, signal, slug, contract, price, shares, reason):
        """è®°å½•å…·ä½“çš„äº¤æ˜“è§¦å‘ä¿¡å·åˆ°ç‹¬ç«‹æ–‡ä»¶ (data/trades/)"""
        data_dir = "data/trades"
        os.makedirs(data_dir, exist_ok=True)
        # ä¸æŒä»“ç”Ÿå‘½å‘¨æœŸæ–‡ä»¶è§£è€¦ï¼Œé¿å…ä¸åŒ schema æ±¡æŸ“ trade_historyã€‚
        filename = f"{data_dir}/trade_events_{city_name}.csv"
        
        file_exists = os.path.isfile(filename)
        row = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'local_time': local_time,
            'signal_type': signal,
            'contract_slug': slug,
            'target_asset': contract,
            'execution_price': f"{price:.3f}" if price else "0.000",
            'shares': shares,
            'reasoning': reason,
            'is_dry_run': "TRUE" if self.config.DRY_RUN else "FALSE"
        }
        
        try:
            with open(filename, 'a', newline='', encoding='utf-8') as f:
                fieldnames = ['timestamp', 'local_time', 'signal_type', 'contract_slug', 'target_asset', 'execution_price', 'shares', 'reasoning', 'is_dry_run']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                if not file_exists:
                    writer.writeheader()
                writer.writerow(row)
            logger.info(f"[{preset_name:8}] ğŸ“Š Trade event logged to {filename}")
        except Exception as e:
            logger.error(f"[{preset_name:8}] Failed to write trade log: {e}")

    def _record_data(self, filename, state, prices, signal, reason, guard_state=None):
        """è®°å½•å®æ—¶æ•°æ®åˆ° CSV (å…¨é‡åŸå§‹è®°å½•ï¼Œå¹³é“ºæŠ¥ä»·åˆ—)"""
        if not prices:
            logger.warning(f"[DEBUG] No prices fetched for {filename}")
            
        data_dir = os.path.dirname(filename)
        os.makedirs(data_dir, exist_ok=True)
        
        file_exists = os.path.isfile(filename)
        
        # åŸºç¡€å­—æ®µ (ä»…ä¿ç•™åŸå§‹è¾“å…¥)
        row = {
            'timestamp': state.timestamp,
            'local_time': state.local_time,
            'local_hour': f"{state.local_hour:.2f}",
            'noaa_curr': state.noaa_curr,
            'om_curr': state.om_curr,
            'om_fore': state.om_fore,
            'mn_curr': state.mn_curr,
            'mn_fore': state.mn_fore,
            'signal': signal,
            'reason': reason,
            'fg_locked': guard_state.get('locked') if guard_state else None,
            'fg_risk_count': guard_state.get('risk_count') if guard_state else None,
            'fg_available_sources': guard_state.get('available_sources') if guard_state else None,
            'fg_reason': guard_state.get('reason') if guard_state else None,
            'fg_afternoon_peak': guard_state.get('avg_afternoon_peak') if guard_state else None,
            'fg_night_peak': guard_state.get('avg_night_peak') if guard_state else None,
            'fg_night_peak_time': guard_state.get('latest_risky_peak_utc').strftime("%H:%M") if (guard_state and guard_state.get('latest_risky_peak_utc')) else None,
            'fg_max_bias': guard_state.get('max_bias') if guard_state else None,
            'fg_max_2h_warming': guard_state.get('max_2h_warming') if guard_state else None
        }
        
        # è®°å½• Yes, No çš„ Ask/Bid å’Œ Volume
        price_cols = []
        if prices:
            for title, p_data in prices.items():
                if isinstance(p_data, dict):
                    row[f"{title}_yes_ask"] = p_data.get('yes_ask')
                    row[f"{title}_yes_bid"] = p_data.get('yes_bid')
                    row[f"{title}_no_ask"] = p_data.get('no_ask')
                    row[f"{title}_no_bid"] = p_data.get('no_bid')
                    row[f"{title}_vol"] = p_data.get('vol')
                    price_cols.extend([
                        f"{title}_yes_ask", f"{title}_yes_bid", 
                        f"{title}_no_ask", f"{title}_no_bid", 
                        f"{title}_vol"
                    ])

        with open(filename, 'a', newline='') as f:
            base_fields = [
                'timestamp', 'local_time', 'local_hour', 
                'noaa_curr', 'om_curr', 'om_fore', 'mn_curr', 'mn_fore',
                'signal', 'reason',
                'fg_locked', 'fg_risk_count', 'fg_available_sources', 'fg_reason',
                'fg_afternoon_peak', 'fg_night_peak', 'fg_night_peak_time', 'fg_max_bias', 'fg_max_2h_warming'
            ]
            
            # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œåªæœ‰åœ¨æ‹¿åˆ°æŠ¥ä»·åæ‰åˆ›å»ºå¹¶å†™å…¥ header
            if not file_exists:
                if not prices:
                    logger.warning(f"[{filename}] Skipping first log: No price data to initialize headers.")
                    return
                fieldnames = base_fields + sorted(price_cols)
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
            else:
                # è¯»å–ç¬¬ä¸€è¡Œè·å– header
                with open(filename, 'r') as fr:
                    reader = csv.reader(fr)
                    try:
                        existing_headers = next(reader)
                    except StopIteration:
                        # æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºçš„æƒ…å†µ
                        if not prices: return
                        existing_headers = base_fields + sorted(price_cols)
                        # è¿™é‡Œéœ€è¦é‡æ–°æ‰“å¼€æ–‡ä»¶å†™å…¥ headerï¼Œæˆ–è€…åœ¨æ­¤å¤„å¤„ç†
                        pass 
                
                # å…¼å®¹å¤„ç†ï¼šå¦‚æœç°æœ‰ header æ²¡æœ‰ä»·æ ¼åˆ—ï¼Œä½†åœ¨æ–°è¡Œä¸­æœ‰ï¼Œ
                # å¯¹äºå·²ç»åˆ›å»ºçš„æ–‡ä»¶ï¼Œæˆ‘ä»¬åªèƒ½å¿½ç•¥æˆ–è€…åœ¨æ­¤å¤„è¡¥å……ï¼ˆæ¯”è¾ƒå¤æ‚ï¼‰ã€‚
                # æ—¢ç„¶æˆ‘ä»¬ä¼šåˆ é™¤æ—§æ–‡ä»¶é‡å¯ï¼Œç¡®ä¿ç¬¬ä¸€æ¬¡æœ‰æ•°æ®å³å¯ã€‚
                writer = csv.DictWriter(f, fieldnames=existing_headers, extrasaction='ignore')
            
            writer.writerow(row)

    async def run_parallel(self, presets, interval=30):
        """å¹¶è¡Œè¿è¡Œå¤šä¸ª Preset"""
        logger.info(f"[*] Launching Multi-Location Engine: {presets}")
        logger.info(f"[*] Mode: {'DRY RUN' if self.config.DRY_RUN else 'REAL'}")
        
        # å¯åŠ¨åå°æŒä»“ç›‘æ§ä¸æŠ¥å‘Šä»»åŠ¡
        asyncio.create_task(self.monitor_and_report_loop(presets))
        
        tasks = [self.run_location_loop(p, interval) for p in presets]
        await asyncio.gather(*tasks)

    async def monitor_and_report_loop(self, presets, report_interval_hours=4):
        """æ¯éš” 4 å°æ—¶æ›´æ–°ä¸€æ¬¡çŠ¶æ€å¹¶å‘é€é’‰é’‰æ±‡æ€»æŠ¥å‘Š (å¯åŠ¨æ—¶ç«‹å³æ¨é€ä¸€æ¬¡)"""
        logger.info(f"[*] Postion Monitor Loop Started (Interval: {report_interval_hours}h)")
        
        while True:
            try:
                # 1. æ›´æ–°æ‰€æœ‰åœ°ç‚¹çš„æŒä»“çŠ¶æ€
                logger.info("[ç›‘æ§] æ­£åœ¨æ›´æ–°æ‰€æœ‰åœ°ç‚¹çš„æŒä»“çŠ¶æ€å¹¶å‡†å¤‡æŠ¥å‘Š...")
                for p in presets:
                    self.pos_manager.update_positions_status(p)
                
                # 2. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå¹¶å‘é€
                report_text = self.pos_manager.get_summary_report()
                
                webhook = os.getenv("DINGTALK_WEBHOOK")
                if webhook:
                    payload = {
                        "msgtype": "text",
                        "text": {
                            "content": f"[Beijixing-WeatherBot] ğŸ“Š å®šæœŸæŒä»“æ±‡æ€»æŠ¥å‘Š\n\nå½“å‰æŒä»“çŠ¶æ€\n{report_text}\n\n-- [Robot: Weather Bot]"
                        }
                    }
                    import json
                    logger.info(f"[ç›‘æ§æ¨é€] Payload: {json.dumps(payload, ensure_ascii=False)}")
                    # ä½¿ç”¨ run_in_executor é¿å…åŒæ­¥è¯·æ±‚é˜»å¡å¼‚æ­¥å¾ªç¯
                    def _send():
                        try:
                            # å¢åŠ å¯¹å“åº”çš„æ·±åº¦æ ¡éªŒ
                            r = requests.post(webhook, json=payload, timeout=15)
                            logger.info(f"[ç›‘æ§] é’‰é’‰å“åº”: {r.status_code} - {r.text}")
                        except Exception as e:
                            logger.error(f"[ç›‘æ§] å‘é€è¯·æ±‚å¼‚å¸¸: {e}")

                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(None, _send)
                else:
                    logger.warning("[ç›‘æ§] é’‰é’‰ Webhook æœªé…ç½®ï¼Œæ— æ³•å‘é€æ±‡æ€»æŠ¥å‘Š")

            except Exception as e:
                logger.error(f"[ç›‘æ§] æŠ¥å‘Šå¾ªç¯å¼‚å¸¸: {e}")
            
            # æœ€åç­‰å¾…é—´éš”æ—¶é—´
            await asyncio.sleep(report_interval_hours * 3600)


if __name__ == "__main__":
    import argparse
    from weather_price_monitor import PRESETS

    parser = argparse.ArgumentParser(description="Polymarket å¤©æ°”è‡ªåŠ¨äº¤æ˜“æœºå™¨äºº")
    parser.add_argument(
        "--presets", 
        nargs="+", 
        help="è¦è¿è¡Œçš„åœ°ç‚¹é¢„è®¾ (è‹¥ä¸æŒ‡å®šåˆ™è¯»å– .env ä¸­çš„ ACTIVE_LOCATIONS)"
    )
    parser.add_argument(
        "--interval", 
        type=int, 
        default=30, 
        help="é‡‡æ ·ä¸»å¾ªç¯é—´éš”(ç§’)ï¼Œé»˜è®¤ 30s"
    )
    
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--dry-run', action='store_true', help='å¼ºåˆ¶å¼€å¯ Dry Run æ¨¡æ‹Ÿæ¨¡å¼ (è¦†ç›– .env)')
    group.add_argument('--real', action='store_true', help='å¼ºåˆ¶å¼€å¯å®ç›˜æ¨¡å¼ (è¦†ç›– .env)')
    
    args = parser.parse_args()
    
    # æ¨¡å¼å¼€å…³ï¼šä¼˜å…ˆçº§ CLI > .env
    if args.dry_run:
        QuantConfig.DRY_RUN = True
        logger.info("[CLI] æ¨¡å¼å¼ºåˆ¶åˆ‡æ¢ä¸º: DRY RUN")
    elif args.real:
        QuantConfig.DRY_RUN = False
        logger.info("[CLI] æ¨¡å¼å¼ºåˆ¶åˆ‡æ¢ä¸º: REAL (è¯·æ³¨æ„é£é™©!)")
    
    # è·å–è¿è¡ŒåŸå¸‚ï¼šä¼˜å…ˆçº§ CLI > .env > Default
    active_cities = args.presets or QuantConfig.ACTIVE_LOCATIONS
    
    # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„åŸå¸‚
    valid_cities = [c for c in active_cities if c in PRESETS]
    if not valid_cities:
        logger.error(f"æ²¡æœ‰å¯è¿è¡Œçš„æœ‰æ•ˆåœ°ç‚¹! è¾“å…¥: {active_cities}")
        exit(1)
        
    logger.info(f"[*] å‡†å¤‡å¯åŠ¨åœ°ç‚¹: {valid_cities}")
    
    bot = WeatherBot()
    try:
        asyncio.run(bot.run_parallel(valid_cities, interval=args.interval))
    except KeyboardInterrupt:
        logger.info("[!] æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡...")
